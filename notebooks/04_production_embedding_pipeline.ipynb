{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Part 02: Loading and Embedding the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóíÔ∏è This notebook is divided in 3 sections:\n",
    "1. Loading the Feature Group from the Hopsworks Feature Store\n",
    "2. Embedding the data using the sentence-transformers library\n",
    "3. Saving the model to the Hopsworks Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import streamlit as st\n",
    "import hopsworks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling the Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hopsworks API key from .env file or secrets.toml file\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    HOPSWORKS_API_KEY = os.getenv('HOPSWORKS_API_KEY')\n",
    "    # HOPSWORKS_API_KEY = st.secrets.HOPSWORKS.HOPSWORKS_API_KEY\n",
    "except:\n",
    "    raise Exception('Set environment variable HOPSWORKS_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/47254\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Connected to the Hopsworks Feature Store\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "    fs = project.get_feature_store()\n",
    "    \n",
    "    print(\"Connected to the Hopsworks Feature Store\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_group = fs.get_feature_group(\"papers_info\", version=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-12 09:26:15,385 INFO: USE `paper_whiz_featurestore`\n",
      "2023-06-12 09:26:15,902 INFO: SELECT `fg0`.`id` `id`, `fg0`.`titles` `titles`, `fg0`.`abstracts` `abstracts`, `fg0`.`terms` `terms`, `fg0`.`urls` `urls`\n",
      "FROM `paper_whiz_featurestore`.`papers_info_1` `fg0`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n"
     ]
    }
   ],
   "source": [
    "# Pull the feature group as a Pandas DataFrame\n",
    "df = feature_group.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>terms</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5139</td>\n",
       "      <td>Adversarial Learning for Debiasing Knowledge Graph Embeddings</td>\n",
       "      <td>Knowledge Graphs (KG) are gaining increasing attention in both academia and\\nindustry. Despite their diverse benefits, recent research have identified\\nsocial and cultural biases embedded in the representations learned from KGs.\\nSuch biases can have detrimental consequences on different population and\\nminority groups as applications of KG begin to intersect and interact with\\nsocial spheres. This paper aims at identifying and mitigating such biases in\\nKnowledge Graph (KG) embeddings. As a first step, we explore popularity bias --\\nthe relationship between node popularity and link prediction accuracy. In case\\nof node2vec graph embeddings, we find that prediction accuracy of the embedding\\nis negatively correlated with the degree of the node. However, in case of\\nknowledge-graph embeddings (KGE), we observe an opposite trend. As a second\\nstep, we explore gender bias in KGE, and a careful examination of popular KGE\\nalgorithms suggest that sensitive attribute like the gender of a person can be\\npredicted from the embedding. This implies that such biases in popular KGs is\\ncaptured by the structural properties of the embedding. As a preliminary\\nsolution to debiasing KGs, we introduce a novel framework to filter out the\\nsensitive attribute information from the KG embeddings, which we call FAN\\n(Filtering Adversarial Network). We also suggest the applicability of FAN for\\ndebiasing other network embeddings which could be explored in future work.</td>\n",
       "      <td>[\"cs.LG\",\"cs.AI\",\"cs.SI\",\"stat.ML\"]</td>\n",
       "      <td>http://arxiv.org/abs/2006.16309v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42559</td>\n",
       "      <td>Graph Convolution for Re-ranking in Person Re-identification</td>\n",
       "      <td>Nowadays, deep learning is widely applied to extract features for similarity\\ncomputation in person re-identification (re-ID) and have achieved great\\nsuccess. However, due to the non-overlapping between training and testing IDs,\\nthe difference between the data used for model training and the testing data\\nmakes the performance of learned feature degraded during testing. Hence,\\nre-ranking is proposed to mitigate this issue and various algorithms have been\\ndeveloped. However, most of existing re-ranking methods focus on replacing the\\nEuclidean distance with sophisticated distance metrics, which are not friendly\\nto downstream tasks and hard to be used for fast retrieval of massive data in\\nreal applications. In this work, we propose a graph-based re-ranking method to\\nimprove learned features while still keeping Euclidean distance as the\\nsimilarity metric. Inspired by graph convolution networks, we develop an\\noperator to propagate features over an appropriate graph. Since graph is the\\nessential key for the propagation, two important criteria are considered for\\ndesigning the graph, and three different graphs are explored accordingly.\\nFurthermore, a simple yet effective method is proposed to generate a profile\\nvector for each tracklet in videos, which helps extend our method to video\\nre-ID. Extensive experiments on three benchmark data sets, e.g., Market-1501,\\nDuke, and MARS, demonstrate the effectiveness of our proposed approach.</td>\n",
       "      <td>[\"cs.CV\"]</td>\n",
       "      <td>http://arxiv.org/abs/2107.02220v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>55744</td>\n",
       "      <td>Towards Safe, Explainable, and Regulated Autonomous Driving</td>\n",
       "      <td>There has been recent and growing interest in the development and deployment\\nof autonomous vehicles, encouraged by the empirical successes of powerful\\nartificial intelligence techniques (AI), especially in the applications of deep\\nlearning and reinforcement learning. However, as demonstrated by recent traffic\\naccidents, autonomous driving technology is not fully reliable for safe\\ndeployment. As AI is the main technology behind the intelligent navigation\\nsystems of self-driving vehicles, both the stakeholders and transportation\\nregulators require their AI-driven software architecture to be safe,\\nexplainable, and regulatory compliant. In this paper, we propose a design\\nframework that integrates autonomous control, explainable AI (XAI), and\\nregulatory compliance to address this issue, and then provide an initial\\nvalidation of the framework with a critical analysis in a case study. Moreover,\\nwe describe relevant XAI approaches that can help achieve the goals of the\\nframework.</td>\n",
       "      <td>[\"cs.AI\"]</td>\n",
       "      <td>http://arxiv.org/abs/2111.10518v4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>82384</td>\n",
       "      <td>InfoSSM: Interpretable Unsupervised Learning of Nonparametric State-Space Model for Multi-modal Dynamics</td>\n",
       "      <td>The goal of system identification is to learn about underlying physics\\ndynamics behind the time-series data. To model the probabilistic and\\nnonparametric dynamics model, Gaussian process (GP) have been widely used; GP\\ncan estimate the uncertainty of prediction and avoid over-fitting. Traditional\\nGPSSMs, however, are based on Gaussian transition model, thus often have\\ndifficulty in describing a more complex transition model, e.g. aircraft\\nmotions. To resolve the challenge, this paper proposes a framework using\\nmultiple GP transition models which is capable of describing multi-modal\\ndynamics. Furthermore, we extend the model to the information-theoretic\\nframework, the so-called InfoSSM, by introducing a mutual information\\nregularizer helping the model to learn interpretable and distinguishable\\nmultiple dynamics models. Two illustrative numerical experiments in simple\\nDubins vehicle and high-fidelity flight simulator are presented to demonstrate\\nthe performance and interpretability of the proposed model. Finally, this paper\\nintroduces a framework using InfoSSM with Bayesian filtering for air traffic\\ncontrol tracking.</td>\n",
       "      <td>[\"stat.ML\",\"cs.LG\",\"stat.AP\"]</td>\n",
       "      <td>http://arxiv.org/abs/1809.07109v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3910</td>\n",
       "      <td>SimMIM: A Simple Framework for Masked Image Modeling</td>\n",
       "      <td>This paper presents SimMIM, a simple framework for masked image modeling. We\\nsimplify recently proposed related approaches without special designs such as\\nblock-wise masking and tokenization via discrete VAE or clustering. To study\\nwhat let the masked image modeling task learn good representations, we\\nsystematically study the major components in our framework, and find that\\nsimple designs of each component have revealed very strong representation\\nlearning performance: 1) random masking of the input image with a moderately\\nlarge masked patch size (e.g., 32) makes a strong pre-text task; 2) predicting\\nraw pixels of RGB values by direct regression performs no worse than the patch\\nclassification approaches with complex designs; 3) the prediction head can be\\nas light as a linear layer, with no worse performance than heavier ones. Using\\nViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K by\\npre-training also on this dataset, surpassing previous best approach by +0.6%.\\nWhen applied on a larger model of about 650 million parameters, SwinV2-H, it\\nachieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We\\nalso leverage this approach to facilitate the training of a 3B model\\n(SwinV2-G), that by $40\\times$ less data than that in previous practice, we\\nachieve the state-of-the-art on four representative vision benchmarks. The code\\nand models will be publicly available at https://github.com/microsoft/SimMIM.</td>\n",
       "      <td>[\"cs.CV\"]</td>\n",
       "      <td>http://arxiv.org/abs/2111.09886v2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "0   5139   \n",
       "1  42559   \n",
       "2  55744   \n",
       "3  82384   \n",
       "4   3910   \n",
       "\n",
       "                                                                                                     titles  \\\n",
       "0                                             Adversarial Learning for Debiasing Knowledge Graph Embeddings   \n",
       "1                                              Graph Convolution for Re-ranking in Person Re-identification   \n",
       "2                                               Towards Safe, Explainable, and Regulated Autonomous Driving   \n",
       "3  InfoSSM: Interpretable Unsupervised Learning of Nonparametric State-Space Model for Multi-modal Dynamics   \n",
       "4                                                      SimMIM: A Simple Framework for Masked Image Modeling   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              abstracts  \\\n",
       "0        Knowledge Graphs (KG) are gaining increasing attention in both academia and\\nindustry. Despite their diverse benefits, recent research have identified\\nsocial and cultural biases embedded in the representations learned from KGs.\\nSuch biases can have detrimental consequences on different population and\\nminority groups as applications of KG begin to intersect and interact with\\nsocial spheres. This paper aims at identifying and mitigating such biases in\\nKnowledge Graph (KG) embeddings. As a first step, we explore popularity bias --\\nthe relationship between node popularity and link prediction accuracy. In case\\nof node2vec graph embeddings, we find that prediction accuracy of the embedding\\nis negatively correlated with the degree of the node. However, in case of\\nknowledge-graph embeddings (KGE), we observe an opposite trend. As a second\\nstep, we explore gender bias in KGE, and a careful examination of popular KGE\\nalgorithms suggest that sensitive attribute like the gender of a person can be\\npredicted from the embedding. This implies that such biases in popular KGs is\\ncaptured by the structural properties of the embedding. As a preliminary\\nsolution to debiasing KGs, we introduce a novel framework to filter out the\\nsensitive attribute information from the KG embeddings, which we call FAN\\n(Filtering Adversarial Network). We also suggest the applicability of FAN for\\ndebiasing other network embeddings which could be explored in future work.   \n",
       "1               Nowadays, deep learning is widely applied to extract features for similarity\\ncomputation in person re-identification (re-ID) and have achieved great\\nsuccess. However, due to the non-overlapping between training and testing IDs,\\nthe difference between the data used for model training and the testing data\\nmakes the performance of learned feature degraded during testing. Hence,\\nre-ranking is proposed to mitigate this issue and various algorithms have been\\ndeveloped. However, most of existing re-ranking methods focus on replacing the\\nEuclidean distance with sophisticated distance metrics, which are not friendly\\nto downstream tasks and hard to be used for fast retrieval of massive data in\\nreal applications. In this work, we propose a graph-based re-ranking method to\\nimprove learned features while still keeping Euclidean distance as the\\nsimilarity metric. Inspired by graph convolution networks, we develop an\\noperator to propagate features over an appropriate graph. Since graph is the\\nessential key for the propagation, two important criteria are considered for\\ndesigning the graph, and three different graphs are explored accordingly.\\nFurthermore, a simple yet effective method is proposed to generate a profile\\nvector for each tracklet in videos, which helps extend our method to video\\nre-ID. Extensive experiments on three benchmark data sets, e.g., Market-1501,\\nDuke, and MARS, demonstrate the effectiveness of our proposed approach.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               There has been recent and growing interest in the development and deployment\\nof autonomous vehicles, encouraged by the empirical successes of powerful\\nartificial intelligence techniques (AI), especially in the applications of deep\\nlearning and reinforcement learning. However, as demonstrated by recent traffic\\naccidents, autonomous driving technology is not fully reliable for safe\\ndeployment. As AI is the main technology behind the intelligent navigation\\nsystems of self-driving vehicles, both the stakeholders and transportation\\nregulators require their AI-driven software architecture to be safe,\\nexplainable, and regulatory compliant. In this paper, we propose a design\\nframework that integrates autonomous control, explainable AI (XAI), and\\nregulatory compliance to address this issue, and then provide an initial\\nvalidation of the framework with a critical analysis in a case study. Moreover,\\nwe describe relevant XAI approaches that can help achieve the goals of the\\nframework.   \n",
       "3                                                                                                                                                                                                                                                                                                                                            The goal of system identification is to learn about underlying physics\\ndynamics behind the time-series data. To model the probabilistic and\\nnonparametric dynamics model, Gaussian process (GP) have been widely used; GP\\ncan estimate the uncertainty of prediction and avoid over-fitting. Traditional\\nGPSSMs, however, are based on Gaussian transition model, thus often have\\ndifficulty in describing a more complex transition model, e.g. aircraft\\nmotions. To resolve the challenge, this paper proposes a framework using\\nmultiple GP transition models which is capable of describing multi-modal\\ndynamics. Furthermore, we extend the model to the information-theoretic\\nframework, the so-called InfoSSM, by introducing a mutual information\\nregularizer helping the model to learn interpretable and distinguishable\\nmultiple dynamics models. Two illustrative numerical experiments in simple\\nDubins vehicle and high-fidelity flight simulator are presented to demonstrate\\nthe performance and interpretability of the proposed model. Finally, this paper\\nintroduces a framework using InfoSSM with Bayesian filtering for air traffic\\ncontrol tracking.   \n",
       "4  This paper presents SimMIM, a simple framework for masked image modeling. We\\nsimplify recently proposed related approaches without special designs such as\\nblock-wise masking and tokenization via discrete VAE or clustering. To study\\nwhat let the masked image modeling task learn good representations, we\\nsystematically study the major components in our framework, and find that\\nsimple designs of each component have revealed very strong representation\\nlearning performance: 1) random masking of the input image with a moderately\\nlarge masked patch size (e.g., 32) makes a strong pre-text task; 2) predicting\\nraw pixels of RGB values by direct regression performs no worse than the patch\\nclassification approaches with complex designs; 3) the prediction head can be\\nas light as a linear layer, with no worse performance than heavier ones. Using\\nViT-B, our approach achieves 83.8% top-1 fine-tuning accuracy on ImageNet-1K by\\npre-training also on this dataset, surpassing previous best approach by +0.6%.\\nWhen applied on a larger model of about 650 million parameters, SwinV2-H, it\\nachieves 87.1% top-1 accuracy on ImageNet-1K using only ImageNet-1K data. We\\nalso leverage this approach to facilitate the training of a 3B model\\n(SwinV2-G), that by $40\\times$ less data than that in previous practice, we\\nachieve the state-of-the-art on four representative vision benchmarks. The code\\nand models will be publicly available at https://github.com/microsoft/SimMIM.   \n",
       "\n",
       "                                 terms                               urls  \n",
       "0  [\"cs.LG\",\"cs.AI\",\"cs.SI\",\"stat.ML\"]  http://arxiv.org/abs/2006.16309v2  \n",
       "1                            [\"cs.CV\"]  http://arxiv.org/abs/2107.02220v2  \n",
       "2                            [\"cs.AI\"]  http://arxiv.org/abs/2111.10518v4  \n",
       "3        [\"stat.ML\",\"cs.LG\",\"stat.AP\"]  http://arxiv.org/abs/1809.07109v2  \n",
       "4                            [\"cs.CV\"]  http://arxiv.org/abs/2111.09886v2  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Setting pandas option to display the full content of DataFrame columns without truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-12 09:32:19,576 INFO: Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "921809152a064093b08923e2e9f0bc00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)e9125/.gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fb1a9c07de440e8b2bc3c93ac0439ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "553db191a2c74b28931160c02032ebff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)7e55de9125/README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7f7664b24f04143b172274e748a72e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)55de9125/config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d93793a4a074313a6da7bcbac4a34d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)ce_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa155d0dd17941218779114354015d29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)125/data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6554360777674a3494e47ba01af1538a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec587d67c30346c58b0e996658646aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)nce_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5633301b76754325b1725bfe6d12ed89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "140dcc0496f84d708d15714680d9ba44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)e9125/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01aee59d8f77410c987fb8a9afbb1419",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)okenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "708237e504474bd4b16a0209d3c185bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)9125/train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4397204c9f6445199608afe00d4101e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)7e55de9125/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c6554018a3141adaa69b66c4bdef538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (‚Ä¶)5de9125/modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-12 09:32:43,306 INFO: Use pytorch device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "221cfe5c158c47b593941604cc89ec26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2486 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Our feature we like to encode\n",
    "sentences = df['titles']\n",
    "\n",
    "# Features are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Adversarial Learning for Debiasing Knowledge Graph Embeddings\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Graph Convolution for Re-ranking in Person Re-identification\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Towards Safe, Explainable, and Regulated Autonomous Driving\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: InfoSSM: Interpretable Unsupervised Learning of Nonparametric State-Space Model for Multi-modal Dynamics\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: SimMIM: A Simple Framework for Masked Image Modeling\n",
      "Embedding length: 384\n",
      "\n",
      "Sentence: Open-Vocabulary Temporal Action Detection with Off-the-Shelf Image-Text Features\n",
      "Embedding length: 384\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing embeddings\n",
    "c = 0\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding length:\", len(embedding)) # list of floats\n",
    "    print(\"\")\n",
    "    if c >=10:\n",
    "        break\n",
    "    c +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving sentences and corresponding embeddings\n",
    "with open('../models/titles_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "with open('../models/titles_sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the model to the Hopsworks Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Connected to the Hopsworks Model Registry\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    mr = project.get_model_registry()\n",
    "    \n",
    "    print(\"Connected to the Hopsworks Model Registry\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_sentences = mr.python.create_model(\n",
    "    name=\"titles_sentences\",\n",
    "    description=\"Scientific papers titles\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91d09d5e316149678bf4ff482ba66e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://c.app.hopsworks.ai:443/p/47254/models/titles_sentences/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(name: 'titles_sentences', version: 1)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr_sentences.save(\"../models/titles_sentences.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mr_embeddings = mr.python.create_model(\n",
    "    name=\"titles_embeddings\",\n",
    "    description=\"Scientific papers embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea3c9edc2654666817221719e3ac51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created, explore it at https://c.app.hopsworks.ai:443/p/47254/models/titles_embeddings/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(name: 'titles_embeddings', version: 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mr_embeddings.save(\"../models/titles_embeddings.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperwhiz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
