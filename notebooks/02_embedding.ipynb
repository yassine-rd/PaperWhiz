{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code to generate sentence embeddings using the pre-trained models from the [sentence-transformers](https://www.sbert.net/index.html) library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_DATA_BASE = Path.cwd().parent / \"data\"\n",
    "PATH_SENTENCES = Path.cwd().parent / \"models/sentences\"\n",
    "PATH_EMBEDDINGS = Path.cwd().parent / \"models/embeddings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting pandas option to display the full content of DataFrame columns without truncation\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>terms</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Compositionally Equivariant Representation Learning</td>\n",
       "      <td>Deep learning models often need sufficient supervision (i.e. labelled data)\\nin order to be trained effectively. By contrast, humans can swiftly learn to\\nidentify important anatomy in medical images like MRI and CT scans, with\\nminimal guidance. This recognition capability easily generalises to new images\\nfrom different medical facilities and to new tasks in different settings. This\\nrapid and generalisable learning ability is largely due to the compositional\\nstructure of image patterns in the human brain, which are not well represented\\nin current medical models. In this paper, we study the utilisation of\\ncompositionality in learning more interpretable and generalisable\\nrepresentations for medical image segmentation. Overall, we propose that the\\nunderlying generative factors that are used to generate the medical images\\nsatisfy compositional equivariance property, where each factor is compositional\\n(e.g. corresponds to the structures in human anatomy) and also equivariant to\\nthe task. Hence, a good representation that approximates well the ground truth\\nfactor has to be compositionally equivariant. By modelling the compositional\\nrepresentations with learnable von-Mises-Fisher (vMF) kernels, we explore how\\ndifferent design and learning biases can be used to enforce the representations\\nto be more compositionally equivariant under un-, weakly-, and semi-supervised\\nsettings. Extensive results show that our methods achieve the best performance\\nover several strong baselines on the task of semi-supervised domain-generalised\\nmedical image segmentation. Code will be made publicly available upon\\nacceptance at https://github.com/vios-s.</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "      <td>http://arxiv.org/abs/2306.07783v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception</td>\n",
       "      <td>We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured\\nusing Aria glasses with extensive object, environment, and human level ground\\ntruth. This ADT release contains 200 sequences of real-world activities\\nconducted by Aria wearers in two real indoor scenes with 398 object instances\\n(324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two\\nmonochrome camera streams, one RGB camera stream, two IMU streams; b) complete\\nsensor calibration; c) ground truth data including continuous\\n6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye\\ngaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d)\\nphoto-realistic synthetic renderings. To the best of our knowledge, there is no\\nexisting egocentric dataset with a level of accuracy, photo-realism and\\ncomprehensiveness comparable to ADT. By contributing ADT to the research\\ncommunity, our mission is to set a new standard for evaluation in the\\negocentric machine perception domain, which includes very challenging research\\nproblems such as 3D object detection and tracking, scene reconstruction and\\nunderstanding, sim-to-real learning, human pose prediction - while also\\ninspiring new machine perception tasks for augmented reality (AR) applications.\\nTo kick start exploration of the ADT research use cases, we evaluated several\\nexisting state-of-the-art methods for object detection, segmentation and image\\ntranslation tasks that demonstrate the usefulness of ADT as a benchmarking\\ndataset.</td>\n",
       "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
       "      <td>http://arxiv.org/abs/2306.06362v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mean Shift Mask Transformer for Unseen Object Instance Segmentation</td>\n",
       "      <td>Segmenting unseen objects from images is a critical perception skill that a\\nrobot needs to acquire. In robot manipulation, it can facilitate a robot to\\ngrasp and manipulate unseen objects. Mean shift clustering is a widely used\\nmethod for image segmentation tasks. However, the traditional mean shift\\nclustering algorithm is not differentiable, making it difficult to integrate it\\ninto an end-to-end neural network training framework. In this work, we propose\\nthe Mean Shift Mask Transformer (MSMFormer), a new transformer architecture\\nthat simulates the von Mises-Fisher (vMF) mean shift clustering algorithm,\\nallowing for the joint training and inference of both the feature extractor and\\nthe clustering. Its central component is a hypersphere attention mechanism,\\nwhich updates object queries on a hypersphere. To illustrate the effectiveness\\nof our method, we apply MSMFormer to unseen object instance segmentation. Our\\nexperiments show that MSMFormer achieves competitive performance compared to\\nstate-of-the-art methods for unseen object instance segmentation. The video and\\ncode are available at https://irvlutd.github.io/MSMFormer</td>\n",
       "      <td>['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']</td>\n",
       "      <td>http://arxiv.org/abs/2211.11679v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AerialFormer: Multi-resolution Transformer for Aerial Image Segmentation</td>\n",
       "      <td>Aerial Image Segmentation is a top-down perspective semantic segmentation and\\nhas several challenging characteristics such as strong imbalance in the\\nforeground-background distribution, complex background, intra-class\\nheterogeneity, inter-class homogeneity, and tiny objects. To handle these\\nproblems, we inherit the advantages of Transformers and propose AerialFormer,\\nwhich unifies Transformers at the contracting path with lightweight\\nMulti-Dilated Convolutional Neural Networks (MD-CNNs) at the expanding path.\\nOur AerialFormer is designed as a hierarchical structure, in which Transformer\\nencoder outputs multi-scale features and MD-CNNs decoder aggregates information\\nfrom the multi-scales. Thus, it takes both local and global contexts into\\nconsideration to render powerful representations and high-resolution\\nsegmentation. We have benchmarked AerialFormer on three common datasets\\nincluding iSAID, LoveDA, and Potsdam. Comprehensive experiments and extensive\\nablation studies show that our proposed AerialFormer outperforms previous\\nstate-of-the-art methods with remarkable performance. Our source code will be\\npublicly available upon acceptance.</td>\n",
       "      <td>['cs.CV']</td>\n",
       "      <td>http://arxiv.org/abs/2306.06842v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VPUFormer: Visual Prompt Unified Transformer for Interactive Image Segmentation</td>\n",
       "      <td>The integration of diverse visual prompts like clicks, scribbles, and boxes\\nin interactive image segmentation could significantly facilitate user\\ninteraction as well as improve interaction efficiency. Most existing studies\\nfocus on a single type of visual prompt by simply concatenating prompts and\\nimages as input for segmentation prediction, which suffers from low-efficiency\\nprompt representation and weak interaction issues. This paper proposes a simple\\nyet effective Visual Prompt Unified Transformer (VPUFormer), which introduces a\\nconcise unified prompt representation with deeper interaction to boost the\\nsegmentation performance. Specifically, we design a Prompt-unified Encoder\\n(PuE) by using Gaussian mapping to generate a unified one-dimensional vector\\nfor click, box, and scribble prompts, which well captures users' intentions as\\nwell as provides a denser representation of user prompts. In addition, we\\npresent a Prompt-to-Pixel Contrastive Loss (P2CL) that leverages user feedback\\nto gradually refine candidate semantic features, aiming to bring image semantic\\nfeatures closer to the features that are similar to the user prompt, while\\npushing away those image semantic features that are dissimilar to the user\\nprompt, thereby correcting results that deviate from expectations. On this\\nbasis, our approach injects prompt representations as queries into Dual-cross\\nMerging Attention (DMA) blocks to perform a deeper interaction between image\\nand query inputs. A comprehensive variety of experiments on seven challenging\\ndatasets demonstrates that the proposed VPUFormer with PuE, DMA, and P2CL\\nachieves consistent improvements, yielding state-of-the-art segmentation\\nperformance. Our code will be made publicly available at\\nhttps://github.com/XuZhang1211/VPUFormer.</td>\n",
       "      <td>['cs.CV', 'cs.RO', 'eess.IV']</td>\n",
       "      <td>http://arxiv.org/abs/2306.06656v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            titles  \\\n",
       "0                              Compositionally Equivariant Representation Learning   \n",
       "1  Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception   \n",
       "2              Mean Shift Mask Transformer for Unseen Object Instance Segmentation   \n",
       "3         AerialFormer: Multi-resolution Transformer for Aerial Image Segmentation   \n",
       "4  VPUFormer: Visual Prompt Unified Transformer for Interactive Image Segmentation   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     abstracts  \\\n",
       "0                                                                                                                                        Deep learning models often need sufficient supervision (i.e. labelled data)\\nin order to be trained effectively. By contrast, humans can swiftly learn to\\nidentify important anatomy in medical images like MRI and CT scans, with\\nminimal guidance. This recognition capability easily generalises to new images\\nfrom different medical facilities and to new tasks in different settings. This\\nrapid and generalisable learning ability is largely due to the compositional\\nstructure of image patterns in the human brain, which are not well represented\\nin current medical models. In this paper, we study the utilisation of\\ncompositionality in learning more interpretable and generalisable\\nrepresentations for medical image segmentation. Overall, we propose that the\\nunderlying generative factors that are used to generate the medical images\\nsatisfy compositional equivariance property, where each factor is compositional\\n(e.g. corresponds to the structures in human anatomy) and also equivariant to\\nthe task. Hence, a good representation that approximates well the ground truth\\nfactor has to be compositionally equivariant. By modelling the compositional\\nrepresentations with learnable von-Mises-Fisher (vMF) kernels, we explore how\\ndifferent design and learning biases can be used to enforce the representations\\nto be more compositionally equivariant under un-, weakly-, and semi-supervised\\nsettings. Extensive results show that our methods achieve the best performance\\nover several strong baselines on the task of semi-supervised domain-generalised\\nmedical image segmentation. Code will be made publicly available upon\\nacceptance at https://github.com/vios-s.   \n",
       "1                                                                                                                                                                                                                                                                      We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured\\nusing Aria glasses with extensive object, environment, and human level ground\\ntruth. This ADT release contains 200 sequences of real-world activities\\nconducted by Aria wearers in two real indoor scenes with 398 object instances\\n(324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two\\nmonochrome camera streams, one RGB camera stream, two IMU streams; b) complete\\nsensor calibration; c) ground truth data including continuous\\n6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye\\ngaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d)\\nphoto-realistic synthetic renderings. To the best of our knowledge, there is no\\nexisting egocentric dataset with a level of accuracy, photo-realism and\\ncomprehensiveness comparable to ADT. By contributing ADT to the research\\ncommunity, our mission is to set a new standard for evaluation in the\\negocentric machine perception domain, which includes very challenging research\\nproblems such as 3D object detection and tracking, scene reconstruction and\\nunderstanding, sim-to-real learning, human pose prediction - while also\\ninspiring new machine perception tasks for augmented reality (AR) applications.\\nTo kick start exploration of the ADT research use cases, we evaluated several\\nexisting state-of-the-art methods for object detection, segmentation and image\\ntranslation tasks that demonstrate the usefulness of ADT as a benchmarking\\ndataset.   \n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Segmenting unseen objects from images is a critical perception skill that a\\nrobot needs to acquire. In robot manipulation, it can facilitate a robot to\\ngrasp and manipulate unseen objects. Mean shift clustering is a widely used\\nmethod for image segmentation tasks. However, the traditional mean shift\\nclustering algorithm is not differentiable, making it difficult to integrate it\\ninto an end-to-end neural network training framework. In this work, we propose\\nthe Mean Shift Mask Transformer (MSMFormer), a new transformer architecture\\nthat simulates the von Mises-Fisher (vMF) mean shift clustering algorithm,\\nallowing for the joint training and inference of both the feature extractor and\\nthe clustering. Its central component is a hypersphere attention mechanism,\\nwhich updates object queries on a hypersphere. To illustrate the effectiveness\\nof our method, we apply MSMFormer to unseen object instance segmentation. Our\\nexperiments show that MSMFormer achieves competitive performance compared to\\nstate-of-the-art methods for unseen object instance segmentation. The video and\\ncode are available at https://irvlutd.github.io/MSMFormer   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Aerial Image Segmentation is a top-down perspective semantic segmentation and\\nhas several challenging characteristics such as strong imbalance in the\\nforeground-background distribution, complex background, intra-class\\nheterogeneity, inter-class homogeneity, and tiny objects. To handle these\\nproblems, we inherit the advantages of Transformers and propose AerialFormer,\\nwhich unifies Transformers at the contracting path with lightweight\\nMulti-Dilated Convolutional Neural Networks (MD-CNNs) at the expanding path.\\nOur AerialFormer is designed as a hierarchical structure, in which Transformer\\nencoder outputs multi-scale features and MD-CNNs decoder aggregates information\\nfrom the multi-scales. Thus, it takes both local and global contexts into\\nconsideration to render powerful representations and high-resolution\\nsegmentation. We have benchmarked AerialFormer on three common datasets\\nincluding iSAID, LoveDA, and Potsdam. Comprehensive experiments and extensive\\nablation studies show that our proposed AerialFormer outperforms previous\\nstate-of-the-art methods with remarkable performance. Our source code will be\\npublicly available upon acceptance.   \n",
       "4  The integration of diverse visual prompts like clicks, scribbles, and boxes\\nin interactive image segmentation could significantly facilitate user\\ninteraction as well as improve interaction efficiency. Most existing studies\\nfocus on a single type of visual prompt by simply concatenating prompts and\\nimages as input for segmentation prediction, which suffers from low-efficiency\\nprompt representation and weak interaction issues. This paper proposes a simple\\nyet effective Visual Prompt Unified Transformer (VPUFormer), which introduces a\\nconcise unified prompt representation with deeper interaction to boost the\\nsegmentation performance. Specifically, we design a Prompt-unified Encoder\\n(PuE) by using Gaussian mapping to generate a unified one-dimensional vector\\nfor click, box, and scribble prompts, which well captures users' intentions as\\nwell as provides a denser representation of user prompts. In addition, we\\npresent a Prompt-to-Pixel Contrastive Loss (P2CL) that leverages user feedback\\nto gradually refine candidate semantic features, aiming to bring image semantic\\nfeatures closer to the features that are similar to the user prompt, while\\npushing away those image semantic features that are dissimilar to the user\\nprompt, thereby correcting results that deviate from expectations. On this\\nbasis, our approach injects prompt representations as queries into Dual-cross\\nMerging Attention (DMA) blocks to perform a deeper interaction between image\\nand query inputs. A comprehensive variety of experiments on seven challenging\\ndatasets demonstrates that the proposed VPUFormer with PuE, DMA, and P2CL\\nachieves consistent improvements, yielding state-of-the-art segmentation\\nperformance. Our code will be made publicly available at\\nhttps://github.com/XuZhang1211/VPUFormer.   \n",
       "\n",
       "                                  terms                               urls  \n",
       "0                    ['cs.CV', 'cs.LG']  http://arxiv.org/abs/2306.07783v1  \n",
       "1           ['cs.CV', 'cs.AI', 'cs.LG']  http://arxiv.org/abs/2306.06362v2  \n",
       "2  ['cs.CV', 'cs.AI', 'cs.LG', 'cs.RO']  http://arxiv.org/abs/2211.11679v2  \n",
       "3                             ['cs.CV']  http://arxiv.org/abs/2306.06842v1  \n",
       "4         ['cs.CV', 'cs.RO', 'eess.IV']  http://arxiv.org/abs/2306.06656v1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv(PATH_DATA_BASE / 'filtered_data.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence-transformers models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a sentence-transformers model?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It maps sentences & paragraphs to a N dimensional dense vector space and can be used for tasks like clustering or semantic search."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### all-MiniLM-L6-v2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MiniLM is a smaller variant of the BERT model which has been designed to provide high-quality language understanding capabilities while being significantly smaller and more efficient. The \"`all-MiniLM-L6-v2`\" model refers to a specific configuration of the MiniLM model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some reasons why I have chosen this model for my project:\n",
    "\n",
    "- Efficiency: MiniLM models are smaller and faster than full-size BERT models, which can be a major advantage if you're working on a project with limited computational resources or if you need to process large amounts of data quickly.\n",
    "- Performance: Despite their smaller size, MiniLM models often perform at a comparable level to full-size BERT models on a variety of NLP tasks. This means that you can often use a MiniLM model without sacrificing much in the way of performance. In fact, the `Performance Sentence Embeddings` metric which is the average performance on encoding sentences over 14 diverse tasks from different domains is `68.06` for the `all-MiniLM-L6-v2` model, which is very good to start with.\n",
    "- Ease of Use: If you're using a library like Hugging Face's Transformers, it can be relatively straightforward to load a pre-trained MiniLM model and fine-tune it for your specific task.\n",
    "- Lower Memory Requirements: Given its smaller size, MiniLM requires less memory for training and inference. This could be a crucial factor if you're working with limited hardware resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Our feature we like to encode\n",
    "sentences = dataset['titles']\n",
    "\n",
    "# Features are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: Compositionally Equivariant Representation Learning\n",
      "Embedding dimension: 384\n",
      "Title length: 51\n",
      "\n",
      "Sentence: Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception\n",
      "Embedding dimension: 384\n",
      "Title length: 79\n",
      "\n",
      "Sentence: Mean Shift Mask Transformer for Unseen Object Instance Segmentation\n",
      "Embedding dimension: 384\n",
      "Title length: 67\n",
      "\n",
      "Sentence: AerialFormer: Multi-resolution Transformer for Aerial Image Segmentation\n",
      "Embedding dimension: 384\n",
      "Title length: 72\n",
      "\n",
      "Sentence: VPUFormer: Visual Prompt Unified Transformer for Interactive Image Segmentation\n",
      "Embedding dimension: 384\n",
      "Title length: 79\n",
      "\n",
      "Sentence: AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder\n",
      "Embedding dimension: 384\n",
      "Title length: 73\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the embeddings\n",
    "c = 0\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding dimension:\", len(embedding))\n",
    "    print(\"Title length:\", len(sentence))\n",
    "    print(\"\")\n",
    "\n",
    "    if c >=5:\n",
    "        break\n",
    "    c +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Saving sentences and corresponding embeddings\n",
    "with open(PATH_EMBEDDINGS / 'embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "with open(PATH_SENTENCES / 'sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(sentences, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Language models'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_you_like = input(\"Enter your topic of interest here ðŸ‘‡ \\n\")\n",
    "paper_you_like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import util\n",
    "cosine_scores = util.cos_sim(embeddings, model.encode(paper_you_like))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[0.7386],\n",
       "        [0.7290],\n",
       "        [0.7224],\n",
       "        [0.7080],\n",
       "        [0.7038]]),\n",
       "indices=tensor([[42238],\n",
       "        [42288],\n",
       "        [42625],\n",
       "        [42476],\n",
       "        [42319]]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "top_similar_papers = torch.topk(cosine_scores,dim=0, k=5,sorted=True)\n",
    "top_similar_papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Large Language Models as Tool Makers\n",
      "Large Language Model Programs\n",
      "Knowledge is a Region in Weight Space for Fine-tuned Language Models\n",
      "Fine-Tuning Language Models with Just Forward Passes\n",
      "On the Creativity of Large Language Models\n"
     ]
    }
   ],
   "source": [
    "for i in top_similar_papers.indices:\n",
    "#     print(i)\n",
    "    print(sentences[i.item()])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperwhiz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
