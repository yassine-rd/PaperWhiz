{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# - Part 01: Initial Data Scraping and Loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗒️ This notebook is divided in 4 sections:\n",
    "1. Scraping the arXiv website for scientific papers using the arXiv API,\n",
    "2. Performing some basic data cleaning and feature engineering,\n",
    "3. Connect to the Hopsworks feature store,\n",
    "4. Create feature groups and upload them to the feature store."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### arXiv Scraping"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we scrape the arXiv website for papers in the category \"cs.CV\" (Computer Vision), \"stat.ML\" / \"cs.LG\" (Machine Learning) and \"cs.AI\" (Artificial Intelligence). The papers are then saved in a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by defining a list of keywords that we will use to query the arXiv API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_keywords = [\n",
    "    \"\\\"image segmentation\\\"\",\n",
    "    \"\\\"self-supervised learning\\\"\",\n",
    "    \"\\\"representation learning\\\"\",\n",
    "    \"\\\"image generation\\\"\",\n",
    "    \"\\\"object detection\\\"\",\n",
    "    \"\\\"transfer learning\\\"\",\n",
    "    \"\\\"transformers\\\"\",\n",
    "    \"\\\"adversarial training\",\n",
    "    \"\\\"generative adversarial networks\\\"\",\n",
    "    \"\\\"model compressions\\\"\",\n",
    "    \"\\\"image segmentation\\\"\",\n",
    "    \"\\\"few-shot learning\\\"\",\n",
    "    \"\\\"natural language\\\"\",\n",
    "    \"\\\"graph\\\"\",\n",
    "    \"\\\"colorization\\\"\",\n",
    "    \"\\\"depth estimation\\\"\",\n",
    "    \"\\\"point cloud\\\"\",\n",
    "    \"\\\"structured data\\\"\",\n",
    "    \"\\\"optical flow\\\"\",\n",
    "    \"\\\"reinforcement learning\\\"\",\n",
    "    \"\\\"super resolution\\\"\",\n",
    "    \"\\\"attention\\\"\",\n",
    "    \"\\\"tabular\\\"\",\n",
    "    \"\\\"unsupervised learning\\\"\",\n",
    "    \"\\\"semi-supervised learning\\\"\",\n",
    "    \"\\\"explainable\\\"\",\n",
    "    \"\\\"radiance field\\\"\",\n",
    "    \"\\\"decision tree\\\"\",\n",
    "    \"\\\"time series\\\"\",\n",
    "    \"\\\"molecule\\\"\",\n",
    "    \"\\\"large language models\\\"\",\n",
    "    \"\\\"llms\\\"\",\n",
    "    \"\\\"language models\\\"\",\n",
    "    \"\\\"image classification\\\"\",\n",
    "    \"\\\"document image classification\\\"\",\n",
    "    \"\\\"encoder\\\"\",\n",
    "    \"\\\"decoder\\\"\",\n",
    "    \"\\\"multimodal\\\"\",\n",
    "    \"\\\"multimodal deep learning\\\"\",\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterwards, we define a function that creates a search object using the given query. It sets the maximum number of results for each category to 6000 and sorts them by the last updated date. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = arxiv.Client(num_retries=20, page_size=500)\n",
    "\n",
    "\n",
    "def query_with_keywords(query) -> tuple:\n",
    "    \"\"\"\n",
    "    Query the arXiv API for research papers based on a specific query and filter results by selected categories.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query to be used for fetching research papers from arXiv.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: A tuple containing three lists - terms, titles, and abstracts of the filtered research papers.\n",
    "        \n",
    "            terms (list): A list of lists, where each inner list contains the categories associated with a research paper.\n",
    "            titles (list): A list of titles of the research papers.\n",
    "            abstracts (list): A list of abstracts (summaries) of the research papers.\n",
    "            urls (list): A list of URLs for the papers' detail page on the arXiv website.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a search object with the query and sorting parameters.\n",
    "    search = arxiv.Search(\n",
    "        query=query,\n",
    "        max_results=6000,\n",
    "        sort_by=arxiv.SortCriterion.LastUpdatedDate\n",
    "    )\n",
    "    \n",
    "    # Initialize empty lists for terms, titles, abstracts, and urls.\n",
    "    terms = []\n",
    "    titles = []\n",
    "    abstracts = []\n",
    "    urls = []\n",
    "\n",
    "    # For each result in the search...\n",
    "    for res in tqdm(client.results(search), desc=query):\n",
    "        # Check if the primary category of the result is in the specified list.\n",
    "        if res.primary_category in [\"cs.CV\", \"stat.ML\", \"cs.LG\", \"cs.AI\"]:\n",
    "            # If it is, append the result's categories, title, summary, and url to their respective lists.\n",
    "            terms.append(res.categories)\n",
    "            titles.append(res.title)\n",
    "            abstracts.append(res.summary)\n",
    "            urls.append(res.entry_id)\n",
    "\n",
    "    # Return the four lists.\n",
    "    return terms, titles, abstracts, urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"image segmentation\": 3180it [00:47, 67.40it/s]\n",
      "\"self-supervised learning\": 0it [00:06, ?it/s]\n",
      "\"representation learning\": 6774it [01:51, 60.49it/s]\n",
      "\"image generation\": 2425it [00:37, 64.98it/s]\n",
      "\"object detection\": 7194it [02:04, 57.56it/s]\n",
      "\"transfer learning\": 5477it [01:27, 62.32it/s]\n",
      "\"transformers\": 10000it [02:40, 62.13it/s]\n",
      "\"adversarial training: 0it [00:03, ?it/s]\n",
      "\"generative adversarial networks\": 5879it [01:35, 61.55it/s]\n",
      "\"model compressions\": 781it [00:13, 59.48it/s]\n",
      "\"image segmentation\": 3180it [00:48, 65.95it/s]\n",
      "\"few-shot learning\": 0it [00:03, ?it/s]\n",
      "\"natural language\": 10000it [02:36, 63.89it/s]\n",
      "\"graph\": 10000it [02:23, 69.55it/s]\n",
      "\"colorization\": 10000it [02:29, 66.72it/s]\n",
      "\"depth estimation\": 1344it [00:20, 66.41it/s]\n",
      "\"point cloud\": 4871it [01:14, 65.48it/s]\n",
      "\"structured data\": 2055it [00:37, 54.18it/s]\n",
      "\"optical flow\": 1617it [00:26, 60.53it/s]\n",
      "\"reinforcement learning\": 10000it [02:31, 65.92it/s]\n",
      "\"super resolution\": 3144it [00:47, 66.47it/s]\n",
      "\"attention\": 10000it [02:43, 61.24it/s]\n",
      "\"tabular\": 1592it [00:30, 52.84it/s]\n",
      "\"unsupervised learning\": 2900it [00:42, 68.57it/s]\n",
      "\"semi-supervised learning\": 0it [00:03, ?it/s]\n",
      "\"explainable\": 10000it [02:35, 64.15it/s]\n",
      "\"radiance field\": 683it [00:11, 58.52it/s]\n",
      "\"decision tree\": 2671it [00:40, 66.24it/s]\n",
      "\"time series\": 10000it [02:56, 56.76it/s]\n",
      "\"molecule\": 10000it [02:40, 62.12it/s]\n",
      "\"large language models\": 2304it [00:37, 62.04it/s]\n",
      "\"llms\": 1455it [00:19, 72.95it/s]\n",
      "\"language models\": 10000it [02:41, 62.03it/s]\n",
      "\"image classification\": 6249it [01:54, 54.59it/s]\n",
      "\"document image classification\": 20it [00:03,  5.71it/s]\n",
      "\"encoder\": 10000it [02:38, 62.94it/s]\n",
      "\"decoder\": 10000it [02:33, 65.19it/s]\n",
      "\"multimodal\": 7638it [01:57, 65.13it/s]\n",
      "\"multimodal deep learning\": 79it [00:04, 19.42it/s]\n"
     ]
    }
   ],
   "source": [
    "all_titles = []\n",
    "all_abstracts = []\n",
    "all_terms = []\n",
    "all_urls = []\n",
    "\n",
    "for query in query_keywords:\n",
    "    terms, titles, abstracts, urls = query_with_keywords(query)\n",
    "    all_titles.extend(titles)\n",
    "    all_abstracts.extend(abstracts)\n",
    "    all_terms.extend(terms)\n",
    "    all_urls.extend(urls)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create a pandas.DataFrame object to store the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data = pd.DataFrame({\n",
    "    'titles': all_titles,\n",
    "    'abstracts': all_abstracts,\n",
    "    'terms': all_terms,\n",
    "    'urls': all_urls\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data_indexed = pd.DataFrame({\n",
    "    'titles': all_titles,\n",
    "    'abstracts': all_abstracts,\n",
    "    'terms': all_terms,\n",
    "    'urls': all_urls\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "arxiv_data_indexed.reset_index(inplace=True)\n",
    "arxiv_data_indexed.rename(columns = {'index':'id'}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we preprocess the data collected in the previous section. We start by removing duplicates and then we clean the text by removing punctuation, stopwords and lemmatizing the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>terms</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mean Shift Mask Transformer for Unseen Object Instance Segmentation</td>\n",
       "      <td>Segmenting unseen objects from images is a critical perception skill that a\\nrobot needs to acquire. In robot manipulation, it can facilitate a robot to\\ngrasp and manipulate unseen objects. Mean shift clustering is a widely used\\nmethod for image segmentation tasks. However, the traditional mean shift\\nclustering algorithm is not differentiable, making it difficult to integrate it\\ninto an end-to-end neural network training framework. In this work, we propose\\nthe Mean Shift Mask Transformer (MSMFormer), a new transformer architecture\\nthat simulates the von Mises-Fisher (vMF) mean shift clustering algorithm,\\nallowing for the joint training and inference of both the feature extractor and\\nthe clustering. Its central component is a hypersphere attention mechanism,\\nwhich updates object queries on a hypersphere. To illustrate the effectiveness\\nof our method, we apply MSMFormer to unseen object instance segmentation. Our\\nexperiments show that MSMFormer achieves competitive performance compared to\\nstate-of-the-art methods for unseen object instance segmentation. The video and\\ncode are available at https://irvlutd.github.io/MSMFormer</td>\n",
       "      <td>[cs.CV, cs.AI, cs.LG, cs.RO]</td>\n",
       "      <td>http://arxiv.org/abs/2211.11679v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AerialFormer: Multi-resolution Transformer for Aerial Image Segmentation</td>\n",
       "      <td>Aerial Image Segmentation is a top-down perspective semantic segmentation and\\nhas several challenging characteristics such as strong imbalance in the\\nforeground-background distribution, complex background, intra-class\\nheterogeneity, inter-class homogeneity, and tiny objects. To handle these\\nproblems, we inherit the advantages of Transformers and propose AerialFormer,\\nwhich unifies Transformers at the contracting path with lightweight\\nMulti-Dilated Convolutional Neural Networks (MD-CNNs) at the expanding path.\\nOur AerialFormer is designed as a hierarchical structure, in which Transformer\\nencoder outputs multi-scale features and MD-CNNs decoder aggregates information\\nfrom the multi-scales. Thus, it takes both local and global contexts into\\nconsideration to render powerful representations and high-resolution\\nsegmentation. We have benchmarked AerialFormer on three common datasets\\nincluding iSAID, LoveDA, and Potsdam. Comprehensive experiments and extensive\\nablation studies show that our proposed AerialFormer outperforms previous\\nstate-of-the-art methods with remarkable performance. Our source code will be\\npublicly available upon acceptance.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2306.06842v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VPUFormer: Visual Prompt Unified Transformer for Interactive Image Segmentation</td>\n",
       "      <td>The integration of diverse visual prompts like clicks, scribbles, and boxes\\nin interactive image segmentation could significantly facilitate user\\ninteraction as well as improve interaction efficiency. Most existing studies\\nfocus on a single type of visual prompt by simply concatenating prompts and\\nimages as input for segmentation prediction, which suffers from low-efficiency\\nprompt representation and weak interaction issues. This paper proposes a simple\\nyet effective Visual Prompt Unified Transformer (VPUFormer), which introduces a\\nconcise unified prompt representation with deeper interaction to boost the\\nsegmentation performance. Specifically, we design a Prompt-unified Encoder\\n(PuE) by using Gaussian mapping to generate a unified one-dimensional vector\\nfor click, box, and scribble prompts, which well captures users' intentions as\\nwell as provides a denser representation of user prompts. In addition, we\\npresent a Prompt-to-Pixel Contrastive Loss (P2CL) that leverages user feedback\\nto gradually refine candidate semantic features, aiming to bring image semantic\\nfeatures closer to the features that are similar to the user prompt, while\\npushing away those image semantic features that are dissimilar to the user\\nprompt, thereby correcting results that deviate from expectations. On this\\nbasis, our approach injects prompt representations as queries into Dual-cross\\nMerging Attention (DMA) blocks to perform a deeper interaction between image\\nand query inputs. A comprehensive variety of experiments on seven challenging\\ndatasets demonstrates that the proposed VPUFormer with PuE, DMA, and P2CL\\nachieves consistent improvements, yielding state-of-the-art segmentation\\nperformance. Our code will be made publicly available at\\nhttps://github.com/XuZhang1211/VPUFormer.</td>\n",
       "      <td>[cs.CV, cs.RO, eess.IV]</td>\n",
       "      <td>http://arxiv.org/abs/2306.06656v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder</td>\n",
       "      <td>The recently introduced Segment Anything Model (SAM) combines a clever\\narchitecture and large quantities of training data to obtain remarkable image\\nsegmentation capabilities. However, it fails to reproduce such results for\\nOut-Of-Distribution (OOD) domains such as medical images. Moreover, while SAM\\nis conditioned on either a mask or a set of points, it may be desirable to have\\na fully automatic solution. In this work, we replace SAM's conditioning with an\\nencoder that operates on the same input image. By adding this encoder and\\nwithout further fine-tuning SAM, we obtain state-of-the-art results on multiple\\nmedical images and video benchmarks. This new encoder is trained via gradients\\nprovided by a frozen SAM. For inspecting the knowledge within it, and providing\\na lightweight segmentation solution, we also learn to decode it into a mask by\\na shallow deconvolution network.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2306.06370v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception</td>\n",
       "      <td>We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured\\nusing Aria glasses with extensive object, environment, and human level ground\\ntruth. This ADT release contains 200 sequences of real-world activities\\nconducted by Aria wearers in two real indoor scenes with 398 object instances\\n(324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two\\nmonochrome camera streams, one RGB camera stream, two IMU streams; b) complete\\nsensor calibration; c) ground truth data including continuous\\n6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye\\ngaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d)\\nphoto-realistic synthetic renderings. To the best of our knowledge, there is no\\nexisting egocentric dataset with a level of accuracy, photo-realism and\\ncomprehensiveness comparable to ADT. By contributing ADT to the research\\ncommunity, our mission is to set a new standard for evaluation in the\\negocentric machine perception domain, which includes very challenging research\\nproblems such as 3D object detection and tracking, scene reconstruction and\\nunderstanding, sim-to-real learning, human pose prediction - while also\\ninspiring new machine perception tasks for augmented reality (AR) applications.\\nTo kick start exploration of the ADT research use cases, we evaluated several\\nexisting state-of-the-art methods for object detection, segmentation and image\\ntranslation tasks that demonstrate the usefulness of ADT as a benchmarking\\ndataset.</td>\n",
       "      <td>[cs.CV, cs.AI, cs.LG]</td>\n",
       "      <td>http://arxiv.org/abs/2306.06362v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                            titles  \\\n",
       "0              Mean Shift Mask Transformer for Unseen Object Instance Segmentation   \n",
       "1         AerialFormer: Multi-resolution Transformer for Aerial Image Segmentation   \n",
       "2  VPUFormer: Visual Prompt Unified Transformer for Interactive Image Segmentation   \n",
       "3        AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder   \n",
       "4  Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     abstracts  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Segmenting unseen objects from images is a critical perception skill that a\\nrobot needs to acquire. In robot manipulation, it can facilitate a robot to\\ngrasp and manipulate unseen objects. Mean shift clustering is a widely used\\nmethod for image segmentation tasks. However, the traditional mean shift\\nclustering algorithm is not differentiable, making it difficult to integrate it\\ninto an end-to-end neural network training framework. In this work, we propose\\nthe Mean Shift Mask Transformer (MSMFormer), a new transformer architecture\\nthat simulates the von Mises-Fisher (vMF) mean shift clustering algorithm,\\nallowing for the joint training and inference of both the feature extractor and\\nthe clustering. Its central component is a hypersphere attention mechanism,\\nwhich updates object queries on a hypersphere. To illustrate the effectiveness\\nof our method, we apply MSMFormer to unseen object instance segmentation. Our\\nexperiments show that MSMFormer achieves competitive performance compared to\\nstate-of-the-art methods for unseen object instance segmentation. The video and\\ncode are available at https://irvlutd.github.io/MSMFormer   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Aerial Image Segmentation is a top-down perspective semantic segmentation and\\nhas several challenging characteristics such as strong imbalance in the\\nforeground-background distribution, complex background, intra-class\\nheterogeneity, inter-class homogeneity, and tiny objects. To handle these\\nproblems, we inherit the advantages of Transformers and propose AerialFormer,\\nwhich unifies Transformers at the contracting path with lightweight\\nMulti-Dilated Convolutional Neural Networks (MD-CNNs) at the expanding path.\\nOur AerialFormer is designed as a hierarchical structure, in which Transformer\\nencoder outputs multi-scale features and MD-CNNs decoder aggregates information\\nfrom the multi-scales. Thus, it takes both local and global contexts into\\nconsideration to render powerful representations and high-resolution\\nsegmentation. We have benchmarked AerialFormer on three common datasets\\nincluding iSAID, LoveDA, and Potsdam. Comprehensive experiments and extensive\\nablation studies show that our proposed AerialFormer outperforms previous\\nstate-of-the-art methods with remarkable performance. Our source code will be\\npublicly available upon acceptance.   \n",
       "2  The integration of diverse visual prompts like clicks, scribbles, and boxes\\nin interactive image segmentation could significantly facilitate user\\ninteraction as well as improve interaction efficiency. Most existing studies\\nfocus on a single type of visual prompt by simply concatenating prompts and\\nimages as input for segmentation prediction, which suffers from low-efficiency\\nprompt representation and weak interaction issues. This paper proposes a simple\\nyet effective Visual Prompt Unified Transformer (VPUFormer), which introduces a\\nconcise unified prompt representation with deeper interaction to boost the\\nsegmentation performance. Specifically, we design a Prompt-unified Encoder\\n(PuE) by using Gaussian mapping to generate a unified one-dimensional vector\\nfor click, box, and scribble prompts, which well captures users' intentions as\\nwell as provides a denser representation of user prompts. In addition, we\\npresent a Prompt-to-Pixel Contrastive Loss (P2CL) that leverages user feedback\\nto gradually refine candidate semantic features, aiming to bring image semantic\\nfeatures closer to the features that are similar to the user prompt, while\\npushing away those image semantic features that are dissimilar to the user\\nprompt, thereby correcting results that deviate from expectations. On this\\nbasis, our approach injects prompt representations as queries into Dual-cross\\nMerging Attention (DMA) blocks to perform a deeper interaction between image\\nand query inputs. A comprehensive variety of experiments on seven challenging\\ndatasets demonstrates that the proposed VPUFormer with PuE, DMA, and P2CL\\nachieves consistent improvements, yielding state-of-the-art segmentation\\nperformance. Our code will be made publicly available at\\nhttps://github.com/XuZhang1211/VPUFormer.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The recently introduced Segment Anything Model (SAM) combines a clever\\narchitecture and large quantities of training data to obtain remarkable image\\nsegmentation capabilities. However, it fails to reproduce such results for\\nOut-Of-Distribution (OOD) domains such as medical images. Moreover, while SAM\\nis conditioned on either a mask or a set of points, it may be desirable to have\\na fully automatic solution. In this work, we replace SAM's conditioning with an\\nencoder that operates on the same input image. By adding this encoder and\\nwithout further fine-tuning SAM, we obtain state-of-the-art results on multiple\\nmedical images and video benchmarks. This new encoder is trained via gradients\\nprovided by a frozen SAM. For inspecting the knowledge within it, and providing\\na lightweight segmentation solution, we also learn to decode it into a mask by\\na shallow deconvolution network.   \n",
       "4                                                                                                                                                                                                                                                                      We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured\\nusing Aria glasses with extensive object, environment, and human level ground\\ntruth. This ADT release contains 200 sequences of real-world activities\\nconducted by Aria wearers in two real indoor scenes with 398 object instances\\n(324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two\\nmonochrome camera streams, one RGB camera stream, two IMU streams; b) complete\\nsensor calibration; c) ground truth data including continuous\\n6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye\\ngaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d)\\nphoto-realistic synthetic renderings. To the best of our knowledge, there is no\\nexisting egocentric dataset with a level of accuracy, photo-realism and\\ncomprehensiveness comparable to ADT. By contributing ADT to the research\\ncommunity, our mission is to set a new standard for evaluation in the\\negocentric machine perception domain, which includes very challenging research\\nproblems such as 3D object detection and tracking, scene reconstruction and\\nunderstanding, sim-to-real learning, human pose prediction - while also\\ninspiring new machine perception tasks for augmented reality (AR) applications.\\nTo kick start exploration of the ADT research use cases, we evaluated several\\nexisting state-of-the-art methods for object detection, segmentation and image\\ntranslation tasks that demonstrate the usefulness of ADT as a benchmarking\\ndataset.   \n",
       "\n",
       "                          terms                               urls  \n",
       "0  [cs.CV, cs.AI, cs.LG, cs.RO]  http://arxiv.org/abs/2211.11679v2  \n",
       "1                       [cs.CV]  http://arxiv.org/abs/2306.06842v1  \n",
       "2       [cs.CV, cs.RO, eess.IV]  http://arxiv.org/abs/2306.06656v1  \n",
       "3                       [cs.CV]  http://arxiv.org/abs/2306.06370v1  \n",
       "4         [cs.CV, cs.AI, cs.LG]  http://arxiv.org/abs/2306.06362v1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting pandas option to display the full content of DataFrame columns without truncation\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "arxiv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>terms</th>\n",
       "      <th>urls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Mean Shift Mask Transformer for Unseen Object Instance Segmentation</td>\n",
       "      <td>Segmenting unseen objects from images is a critical perception skill that a\\nrobot needs to acquire. In robot manipulation, it can facilitate a robot to\\ngrasp and manipulate unseen objects. Mean shift clustering is a widely used\\nmethod for image segmentation tasks. However, the traditional mean shift\\nclustering algorithm is not differentiable, making it difficult to integrate it\\ninto an end-to-end neural network training framework. In this work, we propose\\nthe Mean Shift Mask Transformer (MSMFormer), a new transformer architecture\\nthat simulates the von Mises-Fisher (vMF) mean shift clustering algorithm,\\nallowing for the joint training and inference of both the feature extractor and\\nthe clustering. Its central component is a hypersphere attention mechanism,\\nwhich updates object queries on a hypersphere. To illustrate the effectiveness\\nof our method, we apply MSMFormer to unseen object instance segmentation. Our\\nexperiments show that MSMFormer achieves competitive performance compared to\\nstate-of-the-art methods for unseen object instance segmentation. The video and\\ncode are available at https://irvlutd.github.io/MSMFormer</td>\n",
       "      <td>[cs.CV, cs.AI, cs.LG, cs.RO]</td>\n",
       "      <td>http://arxiv.org/abs/2211.11679v2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>AerialFormer: Multi-resolution Transformer for Aerial Image Segmentation</td>\n",
       "      <td>Aerial Image Segmentation is a top-down perspective semantic segmentation and\\nhas several challenging characteristics such as strong imbalance in the\\nforeground-background distribution, complex background, intra-class\\nheterogeneity, inter-class homogeneity, and tiny objects. To handle these\\nproblems, we inherit the advantages of Transformers and propose AerialFormer,\\nwhich unifies Transformers at the contracting path with lightweight\\nMulti-Dilated Convolutional Neural Networks (MD-CNNs) at the expanding path.\\nOur AerialFormer is designed as a hierarchical structure, in which Transformer\\nencoder outputs multi-scale features and MD-CNNs decoder aggregates information\\nfrom the multi-scales. Thus, it takes both local and global contexts into\\nconsideration to render powerful representations and high-resolution\\nsegmentation. We have benchmarked AerialFormer on three common datasets\\nincluding iSAID, LoveDA, and Potsdam. Comprehensive experiments and extensive\\nablation studies show that our proposed AerialFormer outperforms previous\\nstate-of-the-art methods with remarkable performance. Our source code will be\\npublicly available upon acceptance.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2306.06842v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>VPUFormer: Visual Prompt Unified Transformer for Interactive Image Segmentation</td>\n",
       "      <td>The integration of diverse visual prompts like clicks, scribbles, and boxes\\nin interactive image segmentation could significantly facilitate user\\ninteraction as well as improve interaction efficiency. Most existing studies\\nfocus on a single type of visual prompt by simply concatenating prompts and\\nimages as input for segmentation prediction, which suffers from low-efficiency\\nprompt representation and weak interaction issues. This paper proposes a simple\\nyet effective Visual Prompt Unified Transformer (VPUFormer), which introduces a\\nconcise unified prompt representation with deeper interaction to boost the\\nsegmentation performance. Specifically, we design a Prompt-unified Encoder\\n(PuE) by using Gaussian mapping to generate a unified one-dimensional vector\\nfor click, box, and scribble prompts, which well captures users' intentions as\\nwell as provides a denser representation of user prompts. In addition, we\\npresent a Prompt-to-Pixel Contrastive Loss (P2CL) that leverages user feedback\\nto gradually refine candidate semantic features, aiming to bring image semantic\\nfeatures closer to the features that are similar to the user prompt, while\\npushing away those image semantic features that are dissimilar to the user\\nprompt, thereby correcting results that deviate from expectations. On this\\nbasis, our approach injects prompt representations as queries into Dual-cross\\nMerging Attention (DMA) blocks to perform a deeper interaction between image\\nand query inputs. A comprehensive variety of experiments on seven challenging\\ndatasets demonstrates that the proposed VPUFormer with PuE, DMA, and P2CL\\nachieves consistent improvements, yielding state-of-the-art segmentation\\nperformance. Our code will be made publicly available at\\nhttps://github.com/XuZhang1211/VPUFormer.</td>\n",
       "      <td>[cs.CV, cs.RO, eess.IV]</td>\n",
       "      <td>http://arxiv.org/abs/2306.06656v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder</td>\n",
       "      <td>The recently introduced Segment Anything Model (SAM) combines a clever\\narchitecture and large quantities of training data to obtain remarkable image\\nsegmentation capabilities. However, it fails to reproduce such results for\\nOut-Of-Distribution (OOD) domains such as medical images. Moreover, while SAM\\nis conditioned on either a mask or a set of points, it may be desirable to have\\na fully automatic solution. In this work, we replace SAM's conditioning with an\\nencoder that operates on the same input image. By adding this encoder and\\nwithout further fine-tuning SAM, we obtain state-of-the-art results on multiple\\nmedical images and video benchmarks. This new encoder is trained via gradients\\nprovided by a frozen SAM. For inspecting the knowledge within it, and providing\\na lightweight segmentation solution, we also learn to decode it into a mask by\\na shallow deconvolution network.</td>\n",
       "      <td>[cs.CV]</td>\n",
       "      <td>http://arxiv.org/abs/2306.06370v1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception</td>\n",
       "      <td>We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured\\nusing Aria glasses with extensive object, environment, and human level ground\\ntruth. This ADT release contains 200 sequences of real-world activities\\nconducted by Aria wearers in two real indoor scenes with 398 object instances\\n(324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two\\nmonochrome camera streams, one RGB camera stream, two IMU streams; b) complete\\nsensor calibration; c) ground truth data including continuous\\n6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye\\ngaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d)\\nphoto-realistic synthetic renderings. To the best of our knowledge, there is no\\nexisting egocentric dataset with a level of accuracy, photo-realism and\\ncomprehensiveness comparable to ADT. By contributing ADT to the research\\ncommunity, our mission is to set a new standard for evaluation in the\\negocentric machine perception domain, which includes very challenging research\\nproblems such as 3D object detection and tracking, scene reconstruction and\\nunderstanding, sim-to-real learning, human pose prediction - while also\\ninspiring new machine perception tasks for augmented reality (AR) applications.\\nTo kick start exploration of the ADT research use cases, we evaluated several\\nexisting state-of-the-art methods for object detection, segmentation and image\\ntranslation tasks that demonstrate the usefulness of ADT as a benchmarking\\ndataset.</td>\n",
       "      <td>[cs.CV, cs.AI, cs.LG]</td>\n",
       "      <td>http://arxiv.org/abs/2306.06362v1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  \\\n",
       "0   0   \n",
       "1   1   \n",
       "2   2   \n",
       "3   3   \n",
       "4   4   \n",
       "\n",
       "                                                                            titles  \\\n",
       "0              Mean Shift Mask Transformer for Unseen Object Instance Segmentation   \n",
       "1         AerialFormer: Multi-resolution Transformer for Aerial Image Segmentation   \n",
       "2  VPUFormer: Visual Prompt Unified Transformer for Interactive Image Segmentation   \n",
       "3        AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder   \n",
       "4  Aria Digital Twin: A New Benchmark Dataset for Egocentric 3D Machine Perception   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     abstracts  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Segmenting unseen objects from images is a critical perception skill that a\\nrobot needs to acquire. In robot manipulation, it can facilitate a robot to\\ngrasp and manipulate unseen objects. Mean shift clustering is a widely used\\nmethod for image segmentation tasks. However, the traditional mean shift\\nclustering algorithm is not differentiable, making it difficult to integrate it\\ninto an end-to-end neural network training framework. In this work, we propose\\nthe Mean Shift Mask Transformer (MSMFormer), a new transformer architecture\\nthat simulates the von Mises-Fisher (vMF) mean shift clustering algorithm,\\nallowing for the joint training and inference of both the feature extractor and\\nthe clustering. Its central component is a hypersphere attention mechanism,\\nwhich updates object queries on a hypersphere. To illustrate the effectiveness\\nof our method, we apply MSMFormer to unseen object instance segmentation. Our\\nexperiments show that MSMFormer achieves competitive performance compared to\\nstate-of-the-art methods for unseen object instance segmentation. The video and\\ncode are available at https://irvlutd.github.io/MSMFormer   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Aerial Image Segmentation is a top-down perspective semantic segmentation and\\nhas several challenging characteristics such as strong imbalance in the\\nforeground-background distribution, complex background, intra-class\\nheterogeneity, inter-class homogeneity, and tiny objects. To handle these\\nproblems, we inherit the advantages of Transformers and propose AerialFormer,\\nwhich unifies Transformers at the contracting path with lightweight\\nMulti-Dilated Convolutional Neural Networks (MD-CNNs) at the expanding path.\\nOur AerialFormer is designed as a hierarchical structure, in which Transformer\\nencoder outputs multi-scale features and MD-CNNs decoder aggregates information\\nfrom the multi-scales. Thus, it takes both local and global contexts into\\nconsideration to render powerful representations and high-resolution\\nsegmentation. We have benchmarked AerialFormer on three common datasets\\nincluding iSAID, LoveDA, and Potsdam. Comprehensive experiments and extensive\\nablation studies show that our proposed AerialFormer outperforms previous\\nstate-of-the-art methods with remarkable performance. Our source code will be\\npublicly available upon acceptance.   \n",
       "2  The integration of diverse visual prompts like clicks, scribbles, and boxes\\nin interactive image segmentation could significantly facilitate user\\ninteraction as well as improve interaction efficiency. Most existing studies\\nfocus on a single type of visual prompt by simply concatenating prompts and\\nimages as input for segmentation prediction, which suffers from low-efficiency\\nprompt representation and weak interaction issues. This paper proposes a simple\\nyet effective Visual Prompt Unified Transformer (VPUFormer), which introduces a\\nconcise unified prompt representation with deeper interaction to boost the\\nsegmentation performance. Specifically, we design a Prompt-unified Encoder\\n(PuE) by using Gaussian mapping to generate a unified one-dimensional vector\\nfor click, box, and scribble prompts, which well captures users' intentions as\\nwell as provides a denser representation of user prompts. In addition, we\\npresent a Prompt-to-Pixel Contrastive Loss (P2CL) that leverages user feedback\\nto gradually refine candidate semantic features, aiming to bring image semantic\\nfeatures closer to the features that are similar to the user prompt, while\\npushing away those image semantic features that are dissimilar to the user\\nprompt, thereby correcting results that deviate from expectations. On this\\nbasis, our approach injects prompt representations as queries into Dual-cross\\nMerging Attention (DMA) blocks to perform a deeper interaction between image\\nand query inputs. A comprehensive variety of experiments on seven challenging\\ndatasets demonstrates that the proposed VPUFormer with PuE, DMA, and P2CL\\nachieves consistent improvements, yielding state-of-the-art segmentation\\nperformance. Our code will be made publicly available at\\nhttps://github.com/XuZhang1211/VPUFormer.   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            The recently introduced Segment Anything Model (SAM) combines a clever\\narchitecture and large quantities of training data to obtain remarkable image\\nsegmentation capabilities. However, it fails to reproduce such results for\\nOut-Of-Distribution (OOD) domains such as medical images. Moreover, while SAM\\nis conditioned on either a mask or a set of points, it may be desirable to have\\na fully automatic solution. In this work, we replace SAM's conditioning with an\\nencoder that operates on the same input image. By adding this encoder and\\nwithout further fine-tuning SAM, we obtain state-of-the-art results on multiple\\nmedical images and video benchmarks. This new encoder is trained via gradients\\nprovided by a frozen SAM. For inspecting the knowledge within it, and providing\\na lightweight segmentation solution, we also learn to decode it into a mask by\\na shallow deconvolution network.   \n",
       "4                                                                                                                                                                                                                                                                      We introduce the Aria Digital Twin (ADT) - an egocentric dataset captured\\nusing Aria glasses with extensive object, environment, and human level ground\\ntruth. This ADT release contains 200 sequences of real-world activities\\nconducted by Aria wearers in two real indoor scenes with 398 object instances\\n(324 stationary and 74 dynamic). Each sequence consists of: a) raw data of two\\nmonochrome camera streams, one RGB camera stream, two IMU streams; b) complete\\nsensor calibration; c) ground truth data including continuous\\n6-degree-of-freedom (6DoF) poses of the Aria devices, object 6DoF poses, 3D eye\\ngaze vectors, 3D human poses, 2D image segmentations, image depth maps; and d)\\nphoto-realistic synthetic renderings. To the best of our knowledge, there is no\\nexisting egocentric dataset with a level of accuracy, photo-realism and\\ncomprehensiveness comparable to ADT. By contributing ADT to the research\\ncommunity, our mission is to set a new standard for evaluation in the\\negocentric machine perception domain, which includes very challenging research\\nproblems such as 3D object detection and tracking, scene reconstruction and\\nunderstanding, sim-to-real learning, human pose prediction - while also\\ninspiring new machine perception tasks for augmented reality (AR) applications.\\nTo kick start exploration of the ADT research use cases, we evaluated several\\nexisting state-of-the-art methods for object detection, segmentation and image\\ntranslation tasks that demonstrate the usefulness of ADT as a benchmarking\\ndataset.   \n",
       "\n",
       "                          terms                               urls  \n",
       "0  [cs.CV, cs.AI, cs.LG, cs.RO]  http://arxiv.org/abs/2211.11679v2  \n",
       "1                       [cs.CV]  http://arxiv.org/abs/2306.06842v1  \n",
       "2       [cs.CV, cs.RO, eess.IV]  http://arxiv.org/abs/2306.06656v1  \n",
       "3                       [cs.CV]  http://arxiv.org/abs/2306.06370v1  \n",
       "4         [cs.CV, cs.AI, cs.LG]  http://arxiv.org/abs/2306.06362v1  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arxiv_data_indexed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 82581 rows in the dataset.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(arxiv_data_indexed)} rows in the dataset.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real-world data is noisy. One of the most commonly observed source of noise is data duplication. Here we notice that our initial dataset has got about 20k duplicate entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 23466 duplicate titles.\n"
     ]
    }
   ],
   "source": [
    "total_duplicate_titles = sum(arxiv_data_indexed[\"titles\"].duplicated())\n",
    "print(f\"There are {total_duplicate_titles} duplicate titles.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding further, we drop these entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 59115 rows in the deduplicated dataset.\n"
     ]
    }
   ],
   "source": [
    "arxiv_data_indexed = arxiv_data_indexed[~arxiv_data_indexed[\"titles\"].duplicated()]\n",
    "print(f\"There are {len(arxiv_data_indexed)} rows in the deduplicated dataset.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to the Hopsworks Feature Store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating a feature group, we need to connect to Hopsworks feature store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import streamlit as st\n",
    "import hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hopsworks API key from .env file or secrets.toml file\n",
    "load_dotenv()\n",
    "\n",
    "try:\n",
    "    HOPSWORKS_API_KEY = os.getenv('HOPSWORKS_API_KEY')\n",
    "    # HOPSWORKS_API_KEY = st.secrets.HOPSWORKS.HOPSWORKS_API_KEY\n",
    "except:\n",
    "    raise Exception('Set environment variable HOPSWORKS_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/47254\n",
      "Connected to the Hopsworks project\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "Connected to the Hopsworks Feature Store\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    project = hopsworks.login(api_key_value=HOPSWORKS_API_KEY)\n",
    "    print(\"Connected to the Hopsworks project\")\n",
    "    \n",
    "    fs = project.get_feature_store()\n",
    "    print(\"Connected to the Hopsworks Feature Store\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating feature groups and uploading them to the Feature Store"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [feature group](https://docs.hopsworks.ai/feature-store-api/latest/generated/feature_group/) can be seen as a collection of conceptually related features. In this case, we will create 1 feature group representing the scientific paper information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_info_fg = fs.get_or_create_feature_group(\n",
    "    name=\"papers_info\",\n",
    "    version=1,\n",
    "    description=\"Scientific papers info for recommendations.\",\n",
    "    primary_key=['id'],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have only specified some metadata for the feature group. It does not store any data or even have a schema defined for the data. To make the feature group persistent, we need to populate it with its associated data using the `insert` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Group created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/47254/fs/46148/fg/60955\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd6ca11c38044e919b336e09f755faa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading Dataframe: 0.00% |          | Rows 0/59115 | Elapsed Time: 00:00 | Remaining Time: ?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: KafkaError{code=TOPIC_AUTHORIZATION_FAILED,val=29,str=\"Unable to produce message: Broker: Topic authorization failed\"}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    paper_info_fg.insert(arxiv_data_indexed, overwrite=True)\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_descriptions = [\n",
    "    {\"name\": \"id\", \"description\": \"Scientific paper IDs\"}, \n",
    "    {\"name\": \"titles\", \"description\": \"Scientific paper titles\"}, \n",
    "    {\"name\": \"abstracts\", \"description\": \"Scientific paper abstracts\"}, \n",
    "    {\"name\": \"terms\", \"description\": \"Scientific paper categories\"}, \n",
    "    {\"name\": \"urls\", \"description\": \"URLs to scientific paper detail pages\"}, \n",
    "]\n",
    "\n",
    "for desc in feature_descriptions: \n",
    "    paper_info_fg.update_feature_description(desc[\"name\"], desc[\"description\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature group is now accessible and searchable in the UI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperwhiz",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
